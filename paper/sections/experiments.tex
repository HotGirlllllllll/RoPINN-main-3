% !TEX root = ../main.tex
\section{EXPERIMENTAL RESULTS}
\subsection{Research Questions}
We organize experiments around three questions:
\begin{itemize}
  \item \textbf{RQ1:} Does RoPINN-ResFF improve the core reaction benchmark under the same training budget?
  \item \textbf{RQ2:} Is the gain attributable to the backbone change under controlled ablations?
  \item \textbf{RQ3:} Does the method transfer across PDE types beyond reaction?
\end{itemize}

\subsection{Experimental Protocol}
\textbf{Benchmarks.} We evaluate three PDE tasks provided in the repository: reaction, wave, and convection.

\textbf{Metrics.} Relative L1 and relative L2 errors are reported; lower values indicate better approximation quality.

\textbf{Compute and budget.} Main comparisons use CUDA (\texttt{cuda:0}) and a fixed budget of 1000 optimization iterations.

\textbf{Compared settings.} For reaction, we compare: (i) the original backup PINN baseline, (ii) the current PINN in the modified branch, and (iii) RoPINN-ResFF. For wave and convection, we compare PINN versus RoPINN-ResFF.

\textbf{Reproducibility settings.} Multi-seed reports use seed-aligned baseline/ours pairing. Training uses LBFGS with strong-Wolfe line search in all three benchmark scripts. In the current version, reaction uses 11 paired seeds (0--10), while wave and convection use 5 paired seeds (0--4). Table files are generated by \texttt{scripts/paper\_make\_tables.py}, and statistical tests are generated by \texttt{scripts/paper\_significance\_tests.py}.

\subsection{Compute Accounting}
To address fairness concerns beyond fixed-iteration comparison, we report parameter count and end-to-end wall-time under the same iteration budget and device. In the current draft, this profiling is reported on reaction (seed 0) as the main benchmark where statistical gains are established.

\input{tables/table_compute_accounting.tex}

\subsection{Main Reaction Comparison}
Table~\ref{tab:reaction-main} reports the primary reaction results. The table is auto-generated by \texttt{scripts/paper\_make\_tables.py} from \path{results/paper/summary.csv}.

\input{tables/table_reaction_main.tex}

In this single-run comparison, RoPINN-ResFF yields a substantial improvement over the original baseline.

\subsection{Qualitative Visual Comparison}
To complement scalar metrics, Figure~\ref{fig:qualitative-error} compares absolute-error maps between baseline PINN and RoPINN-ResFF on reaction and wave.

\begin{figure}[H]
\centering
\begin{minipage}[t]{0.48\linewidth}
\centering
\includegraphics[width=\linewidth]{../results/1dreaction_PINN_region_optimization_error_paper_curr_pinn.pdf}\\
\footnotesize Reaction: PINN
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\linewidth}
\centering
\includegraphics[width=\linewidth]{../results/1dreaction_PINN_ResFF_region_optimization_error_paper_resff_only.pdf}\\
\footnotesize Reaction: RoPINN-ResFF
\end{minipage}

\vspace{0.6em}

\begin{minipage}[t]{0.48\linewidth}
\centering
\includegraphics[width=\linewidth]{../results/1dwave_PINN_region_optimization_error_paper_wave_pinn.pdf}\\
\footnotesize Wave: PINN
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\linewidth}
\centering
\includegraphics[width=\linewidth]{../results/1dwave_PINN_ResFF_region_optimization_error_paper_wave_resff.pdf}\\
\footnotesize Wave: RoPINN-ResFF
\end{minipage}

\caption{Task-wise qualitative error comparison on reaction and wave. Lower-intensity error regions are more prominent for RoPINN-ResFF. Convection behavior is quantified in Tables~\ref{tab:conv-mseed} and \ref{tab:stat-tests}.}
\label{fig:qualitative-error}
\end{figure}

\subsection{Reaction Multi-Seed Robustness}
To address stochastic variance, we evaluate reaction with 11 random seeds for baseline and RoPINN-ResFF.

\input{tables/table_reaction_mseed.tex}

Compared with baseline mean errors, RoPINN-ResFF achieves substantially lower error and lower variance. The baseline also exhibits occasional failure cases, whereas RoPINN-ResFF remains stable under seed variation. Relative to baseline means, RoPINN-ResFF reduces L1 by 98.38\% and L2 by 96.21\%.

\subsection{Budget Sensitivity on Reaction}
To address concerns that 1000 iterations may be too short for meaningful comparison, we further evaluate reaction under larger budgets (3000 and 5000 iterations) with paired seeds 0--2.

\input{tables/table_reaction_budget.tex}

The advantage of RoPINN-ResFF remains stable at higher budgets: compared with baseline, L1/L2 reductions are 88.53\%/84.28\% at 3000 iterations and 88.64\%/84.44\% at 5000 iterations. Moreover, the baseline and RoPINN-ResFF means change only marginally from 3000 to 5000 iterations, indicating near-plateau behavior in this setting rather than a purely early-iteration artifact.

\subsection{Wave Multi-Seed Generalization}
\input{tables/table_wave_mseed.tex}

In multi-seed statistics, RoPINN-ResFF improves wave mean errors and exhibits lower variance than baseline.

\subsection{Convection Multi-Seed Generalization}
\input{tables/table_convection_mseed.tex}

For convection, RoPINN-ResFF yields higher mean errors than baseline, indicating a clear task mismatch under the current architecture and hyperparameter configuration.

\subsection{Statistical Tests and Confidence Intervals}
To supplement mean$\pm$std reporting, we conduct paired significance analysis on seed-aligned runs (same seed IDs in baseline and ours). Specifically, we report exact two-sided paired randomization tests and paired bootstrap 95\% confidence intervals for
\[
\Delta=\mathrm{mean}(\mathrm{ours}-\mathrm{baseline}),
\]
where negative $\Delta$ indicates improvement.

\input{tables/table_stat_tests.tex}

Across tasks and metrics, confidence intervals characterize both effect direction and effect magnitude. For reaction (11 paired seeds), both metrics are statistically significant under exact paired randomization tests (p=0.00098 for L1 and L2). Wave and convection currently use smaller seed counts; therefore, we keep claims task-specific and avoid over-generalization.

\subsection{Ablation Insight}
For reaction, the primary gain is attributable to the backbone upgrade (ResFF), with consistent error reductions under both single-run and multi-seed reporting.

\subsection{Reproducibility Artifacts}
The repository already stores paper-ready artifacts:
\begin{itemize}
  \item summary tables/plots under \path{results/paper/};
  \item seed-level data and significance results under \path{results/paper/tables/};
  \item run-level metric CSV files for each run tag;
  \item prediction/error/loss PDFs for each benchmark and run tag.
\end{itemize}

These files provide direct traceability from reported numbers to raw outputs.

\subsection{Validity Scope}
Multi-seed evidence is available for all three tasks, but seed counts are asymmetric in the current draft (reaction: 11, wave/convection: 5). The method remains clearly task-dependent: it is strong on reaction and wave, but consistently weaker on convection under the current design.
