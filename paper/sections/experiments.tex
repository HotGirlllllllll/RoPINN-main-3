% !TEX root = ../main.tex
\section{EXPERIMENTAL RESULTS}
\subsection{Research Questions}
We organize experiments around three questions:
\begin{itemize}
  \item \textbf{RQ1:} Does RoPINN-ResFF improve the core reaction benchmark under the same training budget?
  \item \textbf{RQ2:} Is the gain due primarily to the backbone change or to curriculum scheduling?
  \item \textbf{RQ3:} Does the method transfer across PDE types beyond reaction?
\end{itemize}

\subsection{Experimental Protocol}
\textbf{Benchmarks.} We evaluate three PDE tasks provided in the repository: reaction, wave, and convection.

\textbf{Metrics.} Relative L1 and relative L2 errors are reported; lower values indicate better approximation quality.

\textbf{Compute and budget.} Main comparisons use CUDA (\texttt{cuda:0}) and 1000 optimization iterations.

\textbf{Compared settings.} For reaction, we compare: (i) original backup baseline PINN, (ii) current PINN in the modified branch, (iii) RoPINN-ResFF without curriculum, and (iv) RoPINN-ResFF with curriculum. For wave and convection, we compare PINN versus RoPINN-ResFF.

\textbf{Reproducibility settings.} Multi-seed reports use seed-aligned baseline/ours pairing. Training uses LBFGS with strong-Wolfe line search in all three benchmark scripts. Table files are generated by \texttt{scripts/paper\_make\_tables.py}, and statistical tests are generated by \texttt{scripts/paper\_significance\_tests.py}.

\subsection{Main Reaction Comparison}
Table~\ref{tab:reaction-main} reports the primary reaction results. The table is auto-generated by \texttt{scripts/paper\_make\_tables.py} from \path{results/paper/summary.csv}.

\input{tables/table_reaction_main.tex}

In this single-run comparison, RoPINN-ResFF shows a large relative difference versus the original baseline. The curriculum variant is very close but slightly worse than ResFF-only on this task.

\begin{figure}[H]
\centering
\includegraphics[width=0.78\linewidth]{../results/paper/summary_bar.pdf}
\caption{Reaction summary bar chart generated by \texttt{scripts/paper\_collect\_results.py}.}
\label{fig:reaction-summary}
\end{figure}

\subsection{Qualitative Visual Comparison}
To complement scalar metrics, Figure~\ref{fig:qualitative-error} compares absolute-error maps between baseline PINN and RoPINN-ResFF on reaction and wave.

\begin{figure}[H]
\centering
\begin{minipage}[t]{0.48\linewidth}
\centering
\includegraphics[width=\linewidth]{../results/1dreaction_PINN_region_optimization_error_paper_curr_pinn.pdf}\\
\footnotesize Reaction: PINN
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\linewidth}
\centering
\includegraphics[width=\linewidth]{../results/1dreaction_PINN_ResFF_region_optimization_error_paper_resff_only.pdf}\\
\footnotesize Reaction: RoPINN-ResFF
\end{minipage}

\vspace{0.6em}

\begin{minipage}[t]{0.48\linewidth}
\centering
\includegraphics[width=\linewidth]{../results/1dwave_PINN_region_optimization_error_paper_wave_pinn.pdf}\\
\footnotesize Wave: PINN
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\linewidth}
\centering
\includegraphics[width=\linewidth]{../results/1dwave_PINN_ResFF_region_optimization_error_paper_wave_resff.pdf}\\
\footnotesize Wave: RoPINN-ResFF
\end{minipage}

\caption{Task-wise qualitative error comparison on reaction and wave. Lower-intensity error regions are more prominent for RoPINN-ResFF. Convection behavior is quantified in Tables~\ref{tab:conv-mseed} and \ref{tab:stat-tests}.}
\label{fig:qualitative-error}
\end{figure}

\subsection{Reaction Multi-Seed Robustness}
To address stochastic variance, we additionally evaluate reaction with multiple random seeds for the baseline and RoPINN-ResFF.

\input{tables/table_reaction_mseed.tex}

Compared with baseline mean errors, RoPINN-ResFF shows substantially lower error and lower variance. The baseline also exhibits occasional failure cases, while RoPINN-ResFF remains more stable under seed variation.

\subsection{Wave Multi-Seed Generalization}
\input{tables/table_wave_mseed.tex}

In multi-seed statistics, RoPINN-ResFF improves wave mean errors with lower variance than baseline.

\subsection{Convection Multi-Seed Generalization}
\input{tables/table_convection_mseed.tex}

For convection, RoPINN-ResFF yields higher mean errors than baseline, indicating a clear task mismatch under the current architecture and hyperparameter setup.

\subsection{Statistical Tests and Confidence Intervals}
To supplement mean$\pm$std reporting, we conduct paired significance analysis on seed-aligned runs (same seed IDs in baseline and ours). Specifically, we report exact two-sided paired randomization tests and paired bootstrap 95\% confidence intervals for
\[
\Delta=\mathrm{mean}(\mathrm{ours}-\mathrm{baseline}),
\]
where negative $\Delta$ indicates improvement.

\input{tables/table_stat_tests.tex}

Across all tasks/metrics, confidence intervals characterize the direction and magnitude of effects. Significance strength depends on the current seed count and is reported directly in Table~\ref{tab:stat-tests}; we avoid over-claiming beyond what the current statistical power supports.

\subsection{Ablation Insight}
For reaction, the key gain comes from the backbone upgrade (ResFF). Curriculum scheduling provides a controllable training policy but is not the dominant contributor to the best single-run reaction score in this setup.

\subsection{Reproducibility Artifacts}
The repository already stores paper-ready artifacts:
\begin{itemize}
  \item summary tables/plots under \path{results/paper/};
  \item seed-level data and significance results under \path{results/paper/tables/};
  \item run-level metric CSV files for each run tag;
  \item prediction/error/loss PDFs for each benchmark and run tag.
\end{itemize}

These files support direct traceability from reported numbers to raw outputs.

\subsection{Validity Scope}
Multi-seed evidence is now available for all three tasks, which substantially improves statistical reliability. However, the method remains clearly task-dependent: it is strong on reaction and wave, but consistently weaker on convection under the current design.
