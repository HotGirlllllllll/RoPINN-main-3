% !TEX root = ../main.tex
\section{EXPERIMENTAL RESULTS}
\subsection{Research Questions}
We organize experiments around three questions:
\begin{itemize}
  \item \textbf{RQ1:} Does RoPINN-ResFF improve the core reaction benchmark under the same training budget?
  \item \textbf{RQ2:} Is the gain due primarily to the backbone change or to curriculum scheduling?
  \item \textbf{RQ3:} Does the method transfer across PDE types beyond reaction?
\end{itemize}

\subsection{Experimental Protocol}
\textbf{Benchmarks.} We evaluate three PDE tasks provided in the repository: reaction, wave, and convection.

\textbf{Metrics.} Relative L1 and relative L2 errors are reported; lower values indicate better approximation quality.

\textbf{Compute and budget.} Main comparisons use CUDA (\texttt{cuda:0}) and 1000 optimization iterations.

\textbf{Compared settings.} For reaction, we compare: (i) original backup baseline PINN, (ii) current PINN in the modified branch, (iii) RoPINN-ResFF without curriculum, and (iv) RoPINN-ResFF with curriculum. For wave and convection, we compare PINN versus RoPINN-ResFF.

\subsection{Main Reaction Comparison}
Table~\ref{tab:reaction-main} reports the primary reaction results. The table is auto-generated by \texttt{scripts/paper\_make\_tables.py} from \path{results/paper/summary.csv}.

\input{tables/table_reaction_main.tex}

In this single-run comparison, RoPINN-ResFF shows a large relative difference versus the original baseline. The curriculum variant is very close but slightly worse than ResFF-only on this task.

\begin{figure}[H]
\centering
\includegraphics[width=0.78\linewidth]{../results/paper/summary_bar.pdf}
\caption{Reaction summary bar chart generated by \texttt{scripts/paper\_collect\_results.py}.}
\label{fig:reaction-summary}
\end{figure}

\subsection{Reaction Multi-Seed Robustness}
To address stochastic variance, we additionally evaluate reaction with 5 random seeds (0--4) for the baseline and RoPINN-ResFF.

\input{tables/table_reaction_mseed.tex}

Compared with baseline mean errors, RoPINN-ResFF reduces L1 and L2 by about 98.63\% and 96.82\%, respectively. The baseline also exhibits a clear failure case (seed 2, near 0.98 error), while RoPINN-ResFF remains stable across all 5 seeds.

\subsection{Wave Multi-Seed Generalization}
\input{tables/table_wave_mseed.tex}

In 5-seed statistics, RoPINN-ResFF improves wave mean errors by about 80.25\% (L1) and 80.65\% (L2), with lower variance than baseline.

\subsection{Convection Multi-Seed Generalization}
\input{tables/table_convection_mseed.tex}

For convection, RoPINN-ResFF degrades mean errors by about 64.00\% (L1) and 48.47\% (L2), indicating a clear task mismatch under the current architecture and hyperparameter setup.

\subsection{Ablation Insight}
For reaction, the key gain comes from the backbone upgrade (ResFF). Curriculum scheduling provides a controllable training policy but is not the dominant contributor to the best single-run reaction score in this setup.

\subsection{Reproducibility Artifacts}
The repository already stores paper-ready artifacts:
\begin{itemize}
  \item summary tables/plots under \path{results/paper/};
  \item run-level metric CSV files for each run tag;
  \item prediction/error/loss PDFs for each benchmark and run tag.
\end{itemize}

These files support direct traceability from reported numbers to raw outputs.

\subsection{Validity Scope}
Multi-seed evidence is now available for all three tasks, which substantially improves statistical reliability. However, the method remains clearly task-dependent: it is strong on reaction and wave, but consistently weaker on convection under the current design.
