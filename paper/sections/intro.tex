% !TEX root = ../main.tex
\section{INTRODUCTION}
Physics-informed neural networks (PINNs) provide a mesh-free framework for solving partial differential equations (PDEs) by constraining a neural field to satisfy governing equations and boundary/initial conditions \cite{raissi2019pinn}. In practice, however, PINN training is performed on finite collocation sets, whereas PDE constraints are defined on continuous domains. This mismatch can induce hidden violations between sampled points and lead to unstable convergence.

RoPINN mitigates this issue by replacing pointwise residual training with region-wise optimization around collocation points, estimated via Monte Carlo sampling and trust-region calibration \cite{ropinn2024}. Although this design strengthens local training signals, final performance still depends heavily on backbone expressivity and optimization dynamics.

This paper targets that remaining bottleneck. We propose \textbf{RoPINN-ResFF}, a drop-in backbone upgrade for RoPINN that combines residual MLP blocks with random Fourier feature embedding. The goal is to improve representational capacity and training stability without altering RoPINN's core region-optimization principle.

\textbf{Contributions.}
\begin{itemize}
  \item We introduce a residual Fourier-feature backbone that integrates directly into existing RoPINN scripts and training loops.
  \item We provide controlled ablations that isolate backbone effects and show that performance gains are primarily driven by the backbone upgrade in our setup.
  \item We adopt a two-tier evidence protocol under limited compute: reaction is the confirmatory task (11 seeds with formal significance testing), while wave and convection are reported as transfer-oriented directional comparisons (5 seeds each).
  \item We present reproducible fixed-budget evaluations, including both successful transfer (reaction, wave) and a negative case (convection), to clarify the practical scope of the method.
\end{itemize}
