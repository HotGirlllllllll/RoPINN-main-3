% !TEX root = ../main.tex
\section{METHODOLOGY}
\subsection{RoPINN Background}
Given a PDE residual operator $\mathcal{R}[u](x,t)$, PINN training minimizes a weighted sum of residual, boundary, and initial losses:
\begin{equation}
\mathcal{L}(\theta)=\mathcal{L}_{res}+\mathcal{L}_{bc}+\mathcal{L}_{ic}.
\end{equation}
RoPINN replaces single-point residual evaluation with region-wise expectation around each collocation point $z_i=(x_i,t_i)$:
\begin{equation}
\mathcal{L}_{res}^{region} = \frac{1}{N}\sum_{i=1}^{N}\mathbb{E}_{\xi \sim \mathcal{U}(\mathcal{B}_{r_i})} \left[\ell\left(\mathcal{R}[u_\theta](z_i+\xi)\right)\right],
\end{equation}
where $\mathcal{B}_{r_i}$ is a local trust region and $\ell(\cdot)$ is the residual penalty. In code, this expectation is approximated with Monte Carlo sampling (\texttt{sample\_num}) and one-sided or symmetric perturbation (\texttt{sampling\_mode}).

Trust-region scaling follows gradient-statistics calibration:
\begin{equation}
r = \mathrm{clip}\left(\frac{r_0}{v_g}, 0, r_{max}\right),
\end{equation}
where $v_g$ is the normalized gradient-variance statistic computed from recent iterations. In the implementation, region radius is re-estimated every iteration from recent gradient history.

\subsection{RoPINN-ResFF Backbone}
The baseline PINN uses a plain tanh MLP. We replace it with a residual Fourier-feature network (\texttt{PINN\_ResFF}):
\begin{equation}
\phi(x,t) = \left[\sin(2\pi [x,t]B),\;\cos(2\pi [x,t]B)\right],
\end{equation}
where $B\in\mathbb{R}^{2\times d_{ff}}$ is a fixed random matrix scaled by \texttt{ff\_scale}.

Features are projected to hidden width 512, followed by residual blocks:
\begin{equation}
h_{k+1}=\tanh\left(h_k + W_{k,2}\tanh(W_{k,1}h_k+b_{k,1})+b_{k,2}\right),
\end{equation}
and a final linear readout produces $u_\theta(x,t)$. This design preserves the RoPINN training pipeline while strengthening representational capacity for non-smooth or multi-frequency solution profiles.

\begin{algorithm}[H]
\centering
\caption{RoPINN-ResFF training with region optimization.}
\label{alg:ropinn-training}
\begin{tabular}{p{0.95\linewidth}}
\toprule
\textbf{Input:} Collocation set $\mathcal{D}$, initial radius $r_0$, max radius $r_{\max}$, iterations $T$, model parameters $\theta$. \\
\textbf{Output:} Trained parameters $\theta^\star$. \\
1: Initialize $\theta$, optimizer state, and gradient-history buffer $\mathcal{G} \leftarrow \emptyset$. \\
2: Sample residual/boundary/initial points from $\mathcal{D}$. \\
3: Set normalized gradient-variance estimate $v_g \leftarrow 1$. \\
4: \textbf{for} $t=0$ \textbf{to} $T-1$ \textbf{do} \\
\quad 5: Compute trust-region radius $r_t=\mathrm{clip}(r_0/v_g, 0, r_{\max})$. \\
\quad 6: Draw regional perturbations around collocation points (one-sided or symmetric). \\
\quad 7: Evaluate PDE residual, boundary loss, and initial loss; form total loss $\mathcal{L}_t$. \\
\quad 8: Update $\theta$ via LBFGS (strong-Wolfe line search). \\
\quad 9: Update $\mathcal{G}$ and recompute $v_g$ from recent flattened gradients. \\
10: \textbf{end for} \\
11: Return $\theta^\star \leftarrow \theta$. \\
\bottomrule
\end{tabular}
\end{algorithm}

\subsection{Loss Function Choices}
The code supports MSE, Huber, and pseudo-Huber residual penalties. The main paper configuration uses MSE to keep consistency with the original baseline and isolate the effect of architectural changes. Boundary and initial terms remain squared losses.
